---
marp: true
theme: gaia
paginate: true
---

# Abordagem Multiagente na combina√ß√£o de a√ß√µes de patrulhamento preventivo e de atendimento de chamadas policiais

<!-- <style>
img[alt~="right"] {
  display: block;
  margin: 0 auto;
}
</style>

![w:500 right](images/fig2.png) -->
![bg right:40%](images/fig5.png)

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse
<!-- Instituto de Inform√°tica, Universidade Federal do Rio Grande do Sul -->

---

# Resumo do Projeto

<style scoped>
section {
  font-size: 25px;
}
</style>

Este trabalho prop√µe e avalia um modelo de **Aprendizado por Refor√ßo Multiagente (MARL)** para o patrulhamento policial urbano.

- **M√©todo:** Foi integrada uma arquitetura **Dueling DQN** a um **simulador de eventos discretos** que reproduz a opera√ß√£o policial minuto a minuto.
- **Formula√ß√£o:** O problema √© um **MDP multiagente cooperativo**, onde cada patrulha (agente) aprende uma pol√≠tica de posicionamento.
- **Recompensa:** Uma fun√ß√£o **multiobjetivo** que busca conciliar metas conflitantes (tempo de resposta, cobertura de hotspots, etc.).
- **Principal Achado:** O MARL supera o patrulhamento aleat√≥rio e se aproxima de heur√≠sticas especializadas, com destaque para ocorr√™ncias de prioridade intermedi√°ria.

---

# O Problema e os Objetivos

<style scoped>
section {
  font-size: 25px;
}
</style>

## O Desafio Central
Equilibrar dois objetivos conflitantes da atividade policial:
1.  **Patrulhamento Preventivo:** Maximizar a presen√ßa policial em √°reas de alto risco (*hotspots*) para inibir crimes.
2.  **Atendimento Reativo:** Minimizar o tempo de resposta a chamadas de emerg√™ncia.

## A Solu√ß√£o Proposta
Um **sistema multiagente (MARL)** onde as patrulhas s√£o agentes aut√¥nomos que aprendem uma **pol√≠tica de patrulhamento** para otimizar ambos os objetivos simultaneamente.

---

# Metodologia: Vis√£o Geral

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40%](images/fig1.png)

## Ambiente de Simula√ß√£o
- Baseado em **dados reais** do 9¬∫ Batalh√£o de Pol√≠cia Militar (Porto Alegre/RS).
- Utiliza um **grid hexagonal** para representar o espa√ßo geogr√°fico.

## Identifica√ß√£o de Hotspots
- Um modelo preditivo **(XGBoost)**, treinado com dados hist√≥ricos, classifica os hex√°gonos com base no **risco de ocorr√™ncia** de eventos para as pr√≥ximas 2 horas.

---

# Metodologia: Din√¢mica da Simula√ß√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ![bg right:45%](images/fig3.png) -->

## Simula√ß√£o de Eventos Discretos
- Desenvolvida em Python (`simpy`), modela a opera√ß√£o minuto a minuto.
- **Entidade Patrulha:** Evolui por estados (Dispon√≠vel, Deslocamento, Atendimento, etc.).
- **L√≥gica do Despachante:** Regras fixas (mais pr√≥ximo para P1, da pr√≥pria √°rea para P2/P3).
- **O MARL atua** quando a patrulha est√° no estado `DISPON√çVEL`, decidindo para onde ir.


---

# Metodologia: Din√¢mica da Simula√ß√£o

<style>
section {
  font-size: 25px;
}
img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>

![center](images/fig3.png)


---

# Metodologia: Formula√ß√£o do Modelo

<style scoped>
section {
  font-size: 25px;
}
</style>

O problema √© formulado como um **Processo de Decis√£o de Markov (MDP) multiagente e cooperativo**, definido pela tupla:
$\mathcal{M} = \langle \mathcal{A}, \mathcal{S}, \mathcal{U}, P, R, \gamma \rangle$

- **Agentes ($\mathcal{A}$):** As pr√≥prias patrulhas policiais.
- **Estado ($\mathcal{S}$):** Uma representa√ß√£o do ambiente (posi√ß√µes, filas, etc.).
- **A√ß√µes ($\mathcal{U}$):** O conjunto de v√©rtices de patrulhamento que um agente pode escolher.
- **Transi√ß√£o ($P$):** A din√¢mica do simulador, que atualiza o estado a cada minuto.
- **Recompensa ($R$):** Uma recompensa global compartilhada entre todos os agentes.
- **Fator de Desconto ($\gamma$):** Par√¢metro que pondera a import√¢ncia de recompensas futuras.

---

# Metodologia: Aprendizado do Agente

<style scoped>
section {
  font-size: 25px;
}
</style>

O objetivo √© aprender uma pol√≠tica $\pi(u_i|o_i)$ que mapeia a observa√ß√£o local de um agente para a melhor a√ß√£o de patrulhamento.

- **Algoritmo:** **Dueling Deep Q-Network (Dueling DQN)**.
- **Fun√ß√£o Q:** √â aproximada por uma rede neural (MLP).
- **Arquitetura Dueling:** A rede neural tem dois ramos separados:
    1.  Um para estimar o **valor do estado** ($V(s)$).
    2.  Outro para estimar a **vantagem de cada a√ß√£o** ($A(s,a)$).
- **Estabiliza√ß√£o:** O treinamento utiliza um *replay buffer* e uma *rede-alvo* (target network) para estabilizar o aprendizado.

---

# Metodologia: A Observa√ß√£o do Agente ($o_i$)

<style scoped>
section {
  font-size: 25px;
}
</style>

Cada agente recebe um vetor de 19 dimens√µes com informa√ß√µes locais e globais:

- **Informa√ß√µes Temporais:**
  - Codifica√ß√£o c√≠clica do hor√°rio do dia e do dia da semana.
- **Estado do Sistema (Global):**
  - N√∫mero de chamadas em fila por prioridade (`q1, q2, q3`).
  - M√©dias normalizadas de ociosidade, tempo em fila e deslocamento.
- **Estado do Agente (Local):**
  - Posi√ß√£o (v√©rtice) atual do agente.
  - Companhia e tipo da patrulha (One-Hot Encoded).
- **Informa√ß√µes de Risco (Din√¢micas):**
  - Risco do hex√°gono onde o agente est√°.
  - Risco do *hotspot* mais perigoso na sua √°rea de atua√ß√£o.

---

# Metodologia: A Fun√ß√£o de Recompensa ($r_t$)

<style scoped>
section {
  font-size: 25px;
}
</style>

A recompensa √© **global e compartilhada**, refletindo o desempenho do sistema como um todo.

$r_t = \alpha \cdot \Delta \text{atendidos}_t - \lambda_{\text{idle}} \cdot \widetilde{\Delta \text{idle}_t} - \lambda_{\text{resp}} \cdot \widetilde{\Delta \text{resp}_t} - \lambda_{\text{back}} \cdot \widetilde{\Delta \text{backlog}_t}$

- **Componentes:**
    - **$\Delta \text{atendidos}$ (Positivo):** Incentiva o atendimento de chamadas (com peso por prioridade).
    - **$\widetilde{\Delta \text{idle}}$ (Negativo):** Penaliza o tempo que os *hotspots* ficam sem cobertura.
    - **$\widetilde{\Delta \text{resp}}$ (Negativo):** Penaliza o tempo de resposta √†s chamadas.
    - **$\widetilde{\Delta \text{backlog}}$ (Negativo):** Penaliza o n√∫mero de chamadas esperando na fila.

Os hiperpar√¢metros $\alpha$ e $\lambda$s controlam o *trade-off* entre os objetivos.

---

# Valida√ß√£o: Setup Experimental

<style scoped>
section {
  font-size: 25px;
}
</style>

## Estrat√©gias Comparadas
1.  **MARL_8:** A melhor configura√ß√£o encontrada para o modelo MARL ap√≥s v√°rios experimentos.
2.  **BAPS:** Uma heur√≠stica forte (baseada em Otimiza√ß√£o por Col√¥nia de Formigas) usada como baseline de alto desempenho.
3.  **ALEAT√ìRIO:** Uma baseline simples onde as patrulhas escolhem destinos aleatoriamente.

## Condi√ß√µes de Avalia√ß√£o
- Foram realizadas 3 execu√ß√µes de simula√ß√£o com horizonte de 7 dias.
- Para garantir uma compara√ß√£o justa, **todas as estrat√©gias foram avaliadas sob o mesmo conjunto de chamadas geradas**.

---

# Resultados: Compara√ß√£o Geral

<style scoped>
section {
  font-size: 25px;
}
.footnote {
  font-size: 14px;
  position: absolute;
  bottom: 20px;
  left: 30px;
}
</style>

<!-- ## M√©tricas de Desempenho (M√©dias de 3 Execu√ß√µes) -->
| M√©todo    | Ociosidade | Fila | Deslocamento | Tempo Resposta |
| :-------- | :---------- | :---- | :-------------- | :---------------- |
| ALEAT√ìRIO | 4843.14     | 39.52 | **7.03**        | 46.54             |
| BAPS      | **4516.59** | **27.93** | 7.10        | **35.03**         |
| MARL_8    | 4645.42     | 34.47 | 7.25            | 41.71             |

## An√°lise
- A heur√≠stica **BAPS** apresentou o melhor desempenho global.
- O modelo **MARL_8** superou significativamente a baseline **ALEAT√ìRIA** em todas as m√©tricas.
- O resultado do MARL √© promissor, pois se aproxima de uma heur√≠stica forte, validando que o agente aprendeu uma pol√≠tica coerente.

<div class="footnote">* Valores m√©dios de 3 execu√ß√µes de simula√ß√£o independentes.</div>

---

# Resultados: An√°lise por Prioridade de Fila

<!-- ## Tempo M√©dio na Fila de Espera (em minutos) -->
| Prioridade | ALEAT√ìRIO | BAPS    | MARL_8 |
| :--- | :--- | :--- | :--- |
| **Prioridade 1 (Cr√≠tica)** | 5.04 | **0.00** | 0.06 |
| **Prioridade 2 (Interm.)** | 49.82 | 25.73 | **15.22** |
| **Prioridade 3 (Baixa)** | 48.07 | **28.82** | 40.82 |

## An√°lise
- **P1:** BAPS e MARL_8 s√£o quase √≥timos, zerando a fila para chamadas cr√≠ticas.
- **P3:** BAPS √© melhor. O MARL sacrifica o desempenho em baixa prioridade para otimizar as demais, um comportamento esperado e ajust√°vel.
- **P2:** O MARL-8 foi **superior √† heur√≠stica BAPS**, indicando que o agente aprendeu uma pol√≠tica de posicionamento mais eficaz para chamadas de prioridade intermedi√°ria.

---

# Resultados: O Achado Principal (Prioridade 2)

O modelo **MARL_8** apresentou um desempenho **superior** ao da heur√≠stica BAPS para chamadas de prioridade intermedi√°ria.

- **Fila M√©dia (Prioridade 2):**
    - **MARL_8:** **15.22 min**
    - **BAPS:** 25.73 min

## Hip√≥tese
O agente MARL aprendeu uma pol√≠tica de patrulhamento mais sofisticada. Ele parece ter identificado um padr√£o de posicionamento que equilibra melhor a cobertura de zonas de alto risco com a necessidade de estar pr√≥ximo a √°reas de demanda moderada, algo que a heur√≠stica, com suas regras mais r√≠gidas, n√£o captura explicitamente.

---

# Conclus√£o

## Principais Conclus√µes
- A heur√≠stica **BAPS** se mostrou mais eficiente no agregado, mas o **MARL** √© altamente competitivo.
- O **MARL se destacou na prioridade 2**, indicando que aprendeu pol√≠ticas de posicionamento complexas e n√£o √≥bvias.
- √â poss√≠vel obter ganhos operacionais relevantes **ajustando apenas o padr√£o de patrulhamento** (o que o MARL faz), sem alterar as regras de despacho.

## Contribui√ß√µes
- Integra√ß√£o de patrulhamento preventivo e reativo em um √∫nico simulador.
- Formula√ß√£o do problema como MDP multiagente com recompensa multiobjetivo.
- Valida√ß√£o emp√≠rica com dados reais de uma unidade policial brasileira.

---

# Trabalhos Futuros

1.  **Estender o MARL para o Despacho:** Modelar o despachante tamb√©m como um agente de RL, permitindo o aprendizado de pol√≠ticas de despacho din√¢micas, em vez de usar regras fixas.

2.  **Fun√ß√µes de Recompensa Adaptativas:** Investigar recompensas que se ajustem por prioridade, para calibrar de forma mais fina o trade-off entre os diferentes n√≠veis de criticidade das chamadas.

3.  **An√°lise de Robustez e Transferibilidade:** Avaliar o desempenho do modelo em diferentes cen√°rios de demanda (e.g., eventos especiais, crises) e testar a transferibilidade das pol√≠ticas aprendidas para outras cidades ou contextos operacionais.

---

# Obrigado! üôå  
Perguntas?
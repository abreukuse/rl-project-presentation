---
marp: true
theme: gaia
paginate: true
---

# Abordagem Multiagente na combina√ß√£o de a√ß√µes de patrulhamento preventivo e de atendimento de chamadas policiais

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ![w:500 right](images/fig2.png) -->
![bg right:40%](images/fig5.png)

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse
<!-- Instituto de Inform√°tica, Universidade Federal do Rio Grande do Sul -->

<!-- ---

# Resumo do Projeto

<style scoped>
section {
  font-size: 25px;
}
</style>

Este trabalho prop√µe e avalia um modelo de **Aprendizado por Refor√ßo Multiagente (MARL)** para o patrulhamento policial urbano.

- **M√©todo:** Foi integrada uma arquitetura **Dueling DQN** a um **simulador de eventos discretos** que reproduz a opera√ß√£o policial minuto a minuto.
- **Formula√ß√£o:** O problema √© um **MDP multiagente cooperativo**, onde cada patrulha (agente) aprende uma pol√≠tica de posicionamento.
- **Recompensa:** Uma fun√ß√£o **multiobjetivo** que busca conciliar metas conflitantes (tempo de resposta, cobertura de hotspots, etc.).
- **Principal Achado:** O MARL supera o patrulhamento aleat√≥rio e se aproxima de heur√≠sticas especializadas, com destaque para ocorr√™ncias de prioridade intermedi√°ria.
 -->
---

# O Problema e os Objetivos

<style scoped>
section {
  font-size: 25px;
}
</style>

## O Desafio Central
Equilibrar dois objetivos conflitantes da atividade policial:
1.  **Patrulhamento Preventivo:** Maximizar a presen√ßa policial em √°reas de alto risco (*hotspots*) para inibir crimes.
2.  **Atendimento Reativo:** Minimizar o tempo de resposta a chamadas de emerg√™ncia.

## A Solu√ß√£o Proposta
Um **sistema multiagente (MARL)** onde as patrulhas s√£o agentes aut√¥nomos que aprendem uma **pol√≠tica de patrulhamento** para otimizar ambos os objetivos simultaneamente.

---

# Metodologia: Vis√£o Geral

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40% contain](images/fig1.png)

## Ambiente de Simula√ß√£o
- Baseado em **dados reais** do 9¬∫ Batalh√£o de Pol√≠cia Militar (Porto Alegre/RS).
- Utiliza um **grid hexagonal** para representar o espa√ßo geogr√°fico.

## Identifica√ß√£o de Hotspots
- Um modelo preditivo **(XGBoost)**, treinado com dados hist√≥ricos, classifica os hex√°gonos com base no **risco de ocorr√™ncia** de eventos para as pr√≥ximas 2 horas.

---

# Metodologia: Din√¢mica da Simula√ß√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ![bg right:45%](images/fig3.png) -->

## Simula√ß√£o de Eventos Discretos
- Desenvolvida em Python (`simpy`), modela a opera√ß√£o minuto a minuto.
- **Entidade Patrulha:** Evolui por estados (Dispon√≠vel, Deslocamento, Atendimento, etc.).
- **L√≥gica do Despachante:** Regras fixas (mais pr√≥ximo para P1, da pr√≥pria √°rea para P2/P3).
- **O MARL atua** quando a patrulha est√° no estado `DISPON√çVEL`, decidindo para onde ir.
- **Eventos Aleat√≥rios:** A chegada de chamadas √© modelada por uma **distribui√ß√£o de Poisson** (para os intervalos entre chamadas). O tempo de atendimento de cada ocorr√™ncia √© sorteado de uma **distribui√ß√£o exponencial**.

---

# Metodologia: Formula√ß√£o MARL

<style scoped>
section {
  font-size: 25px;
}
</style>

O problema √© formulado como um **Processo de Decis√£o de Markov (MDP) multiagente e cooperativo**, definido pela tupla:
$\mathcal{M} = \langle \mathcal{A}, \mathcal{S}, \mathcal{U}, P, R, \gamma \rangle$

- **Agentes ($\mathcal{A}$):** As pr√≥prias patrulhas policiais.
- **Estado ($\mathcal{S}$):** Uma representa√ß√£o do ambiente (posi√ß√µes, filas, etc.).
- **A√ß√µes ($\mathcal{U}$):** O conjunto de v√©rtices de patrulhamento que um agente pode escolher.
- **Transi√ß√£o ($P$):** A din√¢mica do simulador, que atualiza o estado a cada minuto.
- **Recompensa ($R$):** Uma recompensa global compartilhada entre todos os agentes.
- **Fator de Desconto ($\gamma$):** Par√¢metro que pondera a import√¢ncia de recompensas futuras.

---

# Formula√ß√£o MARL: Agentes e Estados

<style scoped>
section {
  font-size: 25px;
}
</style>

## Agentes (A)
Os **agentes** no modelo s√£o as pr√≥prias **patrulhas policiais**.
- Cada patrulha opera como um agente de decis√£o independente.
- O objetivo √© aprender uma pol√≠tica que contribua para o bem comum do sistema.

## Estado (S) vs. Observa√ß√£o ($o_i$)
- **Estado Global ($\mathcal{S}$):** A "verdade absoluta" do simulador (todas as patrulhas, chamadas, etc.).
- **Observa√ß√£o Local ($o_i$):** A vis√£o **parcial** que cada agente `i` tem do mundo (o vetor de 19 dimens√µes), que √© a entrada para sua rede neural.

---

# Formula√ß√£o MARL: A√ß√µes e Transi√ß√µes

<style scoped>
section {
  font-size: 25px;
}
</style>

## A√ß√µes (U)
O **espa√ßo de a√ß√µes ($\mathcal{U}$)** define o que um agente pode fazer.
- √â **discreto**: a a√ß√£o ($u_i$) √© a escolha de um **v√©rtice de destino** para patrulhamento (hotspots ou quart√©is).

## Transi√ß√£o (P)
A **fun√ß√£o de transi√ß√£o ($P$)** s√£o as "regras da f√≠sica" do ambiente.
- √â a pr√≥pria **din√¢mica do simulador**: processa as a√ß√µes, introduz eventos aleat√≥rios (chamadas) e atualiza o estado do mundo a cada minuto.

---

# Formula√ß√£o MARL: Recompensa (R)

<style scoped>
section {
  font-size: 25px;
}
</style>

A recompensa √© **global e compartilhada** para incentivar a coopera√ß√£o.
$r_t = \alpha \cdot \Delta \text{atendidos}_t - \lambda_{\text{idle}} \cdot \widetilde{\Delta \text{idle}_t} - \lambda_{\text{resp}} \cdot \widetilde{\Delta \text{resp}_t} - \lambda_{\text{back}} \cdot \widetilde{\Delta \text{backlog}_t}$

- **Componentes da Recompensa:**
    - **$\Delta \text{atendidos}_t$ (Positivo):** Recompensa por chamados atendidos no minuto, ponderado pela prioridade. Incentiva a **efici√™ncia**.
    - **$\widetilde{\Delta \text{idle}_t}$ (Negativo):** Penaliza o aumento da ociosidade acumulada nos hotspots. Incentiva a **preven√ß√£o**.
    - **$\widetilde{\Delta \text{resp}_t}$ (Negativo):** Penaliza o aumento do tempo de resposta acumulado (ponderado por prioridade). Incentiva a **agilidade**.
    - **$\widetilde{\Delta \text{backlog}_t}$ (Negativo):** Penaliza o aumento de chamadas esperando na fila. Incentiva a **capacidade do sistema**.
<!-- - Os hiperpar√¢metros $\alpha$ e $\lambda$s controlam o *trade-off* entre esses objetivos. -->

\* Os termos com til ($\widetilde{\cdot}$) representam vers√µes normalizadas dos deltas.

---

# Metodologia: Arquitetura da Rede (Dueling DQN)

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40% contain](images/fig7.png)

A fun√ß√£o de valor $Q(o, u)$ de cada agente √© aproximada por uma rede neural (MLP) com a arquitetura **Dueling DQN**.

Esta arquitetura possui dois "fluxos" separados:
1.  **Fluxo do Valor:** Estima o qu√£o bom √© o estado atual - $V(s)$.
2.  **Fluxo da Vantagem:** Estima a vantagem de cada a√ß√£o naquele estado - $A(s, a)$.

Os dois ramos s√£o combinados para gerar os Q-values finais, o que estabiliza o aprendizado:
$Q(s, a) = V(s) + (A(s, a) - \text{mean}(A(s, a)))$

<!-- ---

# Metodologia: Processo de Treinamento

<style scoped>
section {
  font-size: 25px;
}
</style>

O treinamento utiliza t√©cnicas padr√£o de Deep RL para estabilidade e efici√™ncia:

- **Experience Replay:** As transi√ß√µes $(s, a, r, s')$ s√£o armazenadas em um *replay buffer*. O treinamento √© feito em lotes amostrados aleatoriamente deste buffer para quebrar a correla√ß√£o entre as amostras.

- **Rede-Alvo (Target Network):** Uma segunda rede, com pesos "congelados", √© usada para calcular o valor do estado futuro. Seus pesos s√£o atualizados com menos frequ√™ncia para evitar instabilidade no c√°lculo da perda (loss).

- **Pol√≠tica $\epsilon$-Greedy:** Para balancear explora√ß√£o e explota√ß√£o, o agente escolhe uma a√ß√£o aleat√≥ria com probabilidade $\epsilon$, que decai ao longo do treinamento. -->

---

# Valida√ß√£o: Setup Experimental

<style scoped>
section {
  font-size: 25px;
}
</style>

## Estrat√©gias Comparadas
1.  **MARL_8:** A melhor configura√ß√£o encontrada para o modelo MARL ap√≥s v√°rios experimentos.
2.  **BAPS:** Uma heur√≠stica forte (baseada em Otimiza√ß√£o por Col√¥nia de Formigas) usada como baseline de alto desempenho.
3.  **ALEAT√ìRIO:** Uma baseline simples onde as patrulhas escolhem destinos aleatoriamente.

## Condi√ß√µes de Avalia√ß√£o
- Foram realizadas 3 execu√ß√µes de simula√ß√£o com horizonte de 7 dias.
- Para garantir uma compara√ß√£o justa, **todas as estrat√©gias foram avaliadas sob o mesmo conjunto de chamadas geradas**.

---

# Resultados: Compara√ß√£o Geral

<style scoped>
section {
  font-size: 25px;
}
.footnote {
  font-size: 14px;
  position: absolute;
  bottom: 20px;
  left: 30px;
}
</style>

<!-- ## M√©tricas de Desempenho (M√©dias de 3 Execu√ß√µes) -->
| M√©todo    | Ociosidade | Fila | Deslocamento | Tempo Resposta |
| :-------- | :---------- | :---- | :-------------- | :---------------- |
| ALEAT√ìRIO | 4843.14     | 39.52 | **7.03**        | 46.54             |
| BAPS      | **4516.59** | **27.93** | 7.10        | **35.03**         |
| MARL_8    | 4645.42     | 34.47 | 7.25            | 41.71             |

## An√°lise
<!-- - A heur√≠stica **BAPS** apresentou o melhor desempenho global. -->
- A heur√≠stica **BAPS** (guiada por risco) se confirmou como a mais eficiente no geral, alcan√ßando os menores tempos de resposta, fila e ociosidade.
- O modelo **MARL_8** superou significativamente a baseline **ALEAT√ìRIA** em todas as m√©tricas.
- O resultado do **MARL** √© promissor, pois se aproxima de uma heur√≠stica forte, validando que o agente aprendeu uma pol√≠tica coerente.

<div class="footnote">* Valores m√©dios de 3 execu√ß√µes de simula√ß√£o independentes.</div>

---

# Resultados: An√°lise por Prioridade de Fila

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ## Tempo M√©dio na Fila de Espera (em minutos) -->
| Prioridade | ALEAT√ìRIO | BAPS    | MARL_8 |
| :--- | :--- | :--- | :--- |
| **Prioridade 1 (Cr√≠tica)** | 5.04 | **0.00** | 0.06 |
| **Prioridade 2 (Interm.)** | 49.82 | 25.73 | **15.22** |
| **Prioridade 3 (Baixa)** | 48.07 | **28.82** | 40.82 |

## An√°lise
- **P1:** BAPS e MARL_8 s√£o quase √≥timos, zerando a fila para chamadas cr√≠ticas.
- **P3:** BAPS √© melhor. O MARL sacrifica o desempenho em baixa prioridade para otimizar as demais, um comportamento esperado e ajust√°vel.
- **P2:** O MARL-8 foi **superior √† heur√≠stica BAPS**, indicando que o agente aprendeu uma pol√≠tica de posicionamento mais eficaz para chamadas de prioridade intermedi√°ria.

---

# Resultados: O Achado Principal (Prioridade 2)

<style scoped>
section {
  font-size: 25px;
}
</style>

O modelo **MARL_8** apresentou um desempenho **superior** ao da heur√≠stica BAPS para chamadas de prioridade intermedi√°ria.

- **Fila M√©dia (Prioridade 2):**
    - **MARL_8:** **15.22 min**
    - **BAPS:** 25.73 min

## Hip√≥tese
A abordagem **MARL** aprendeu uma pol√≠tica de patrulhamento mais sofisticada. Ela parece ter identificado um padr√£o de posicionamento que equilibra melhor a cobertura de zonas de alto risco com a necessidade de estar pr√≥ximo a √°reas de demanda moderada, algo que a heur√≠stica, com suas regras mais r√≠gidas, n√£o captura explicitamente.

---

# Conclus√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

## Principais Conclus√µes
- A heur√≠stica **BAPS** se mostrou mais eficiente no agregado, mas o **MARL** √© altamente competitivo.
- O **MARL se destacou na prioridade 2**, indicando que aprendeu pol√≠ticas de posicionamento complexas e n√£o √≥bvias.
- √â poss√≠vel obter ganhos operacionais relevantes **ajustando apenas o padr√£o de patrulhamento** (o que o MARL faz), sem alterar as regras de despacho.

## Contribui√ß√µes
- Integra√ß√£o de patrulhamento preventivo e reativo em um √∫nico simulador.
- Formula√ß√£o do problema como MDP multiagente com recompensa multiobjetivo.
- Valida√ß√£o emp√≠rica com dados reais de uma unidade policial brasileira.

---

# Trabalhos Futuros

<style scoped>
section {
  font-size: 25px;
}
</style>

1.  **Estender o MARL para o Despacho:** Modelar o despachante tamb√©m como um agente de RL, permitindo o aprendizado de pol√≠ticas de despacho din√¢micas, em vez de usar regras fixas.

2.  **Fun√ß√µes de Recompensa Adaptativas:** Investigar recompensas que se ajustem por prioridade, para calibrar de forma mais fina o trade-off entre os diferentes n√≠veis de criticidade das chamadas.

3.  **An√°lise de Robustez e Transferibilidade:** Avaliar o desempenho do modelo em diferentes cen√°rios de demanda (e.g., eventos especiais, crises) e testar a transferibilidade das pol√≠ticas aprendidas para outras cidades ou contextos operacionais.

---

# Obrigado! üôå  
Perguntas?


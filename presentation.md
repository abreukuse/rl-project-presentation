---
marp: true
theme: gaia
paginate: true
---

# Abordagem Multiagente na Combina√ß√£o de A√ß√µes de Patrulhamento Preventivo e de Atendimento de Chamadas Policiais

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40%](images/fig5.png)

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse

---

# O Problema e os Objetivos

<style scoped>
section {
  font-size: 25px;
}
</style>

## O Desafio Central
Equilibrar dois objetivos conflitantes da atividade policial:
1.  **Patrulhamento Preventivo:** Maximizar a presen√ßa policial em √°reas de alto risco (*hotspots*) para inibir crimes.
2.  **Atendimento Reativo:** Minimizar o tempo de resposta a chamadas de emerg√™ncia.

## A Solu√ß√£o Proposta
Um **sistema multiagente (MARL)** onde as patrulhas s√£o agentes aut√¥nomos que aprendem uma **pol√≠tica de patrulhamento** para otimizar ambos os objetivos simultaneamente.

---

# Setup do Ambiente

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40% contain](images/fig1.png)

## Ambiente de Simula√ß√£o
- Baseado em **dados reais** do 9¬∫ Batalh√£o de Pol√≠cia Militar (Porto Alegre/RS).
- Utiliza um **grid hexagonal** para representar o espa√ßo geogr√°fico.

## Identifica√ß√£o de Hotspots
- Um modelo preditivo **(XGBoost)**, treinado com dados hist√≥ricos, classifica os hex√°gonos com base no **risco de ocorr√™ncia** de eventos para as pr√≥ximas 2 horas.

---

<!-- # Metodologia: Din√¢mica da Simula√ß√£o -->
# Din√¢mica da Simula√ß√£o
<style scoped>
section {
  font-size: 25px;
}
/*h2 {
  font-size: 32px;
}*/
</style>

![bg right:40% contain](images/fig8.png)

<!-- ## Simula√ß√£o de Eventos Discretos -->

<!-- O ambiente simula a opera√ß√£o policial com base em eventos estoc√°sticos: -->

### 1. Gera√ß√£o de Chamadas (Estoc√°stico)
<!-- A chegada de novas ocorr√™ncias √© modelada por processos aleat√≥rios: -->
- **Intervalo entre Chamadas:** Segue uma **distribui√ß√£o de Poisson**.
- **Dura√ß√£o do Atendimento:** O tempo para resolver uma ocorr√™ncia segue uma **distribui√ß√£o Exponencial**.
<!-- - **Prioridades:** As chamadas s√£o classificadas em 3 n√≠veis de criticidade. -->

### 2. Despacho e Fila (Regras)
- **Regras de Despacho:** A aloca√ß√£o de viaturas segue regras operacionais fixas:
    - **Prioridade 1:** Patrulha mais pr√≥xima.
    - **Prioridade 2 e 3:** Patrulha da pr√≥pria √°rea (Companhia).
- **Fila de Espera:** Nenhuma patrulha dispon√≠vel.

<!-- - **Fila de Espera:** Se nenhuma patrulha estiver dispon√≠vel, a chamada entra em uma fila FIFO, ordenada por prioridade. -->


---

# Formula√ß√£o MARL

<style scoped>
section {
  font-size: 25px;
}
</style>

O problema √© formulado como um **Processo de Decis√£o de Markov (MDP) multiagente cooperativo**, definido pela tupla:
$\mathcal{M} = \langle \mathcal{A}, \mathcal{S}, \mathcal{U}, P, R, \gamma \rangle$

- **Agentes ($\mathcal{A}$):** O conjunto de agentes (patrulhas).
- **Estado ($\mathcal{S}$):** O espa√ßo de estados global.
- **A√ß√µes ($\mathcal{U}$):** O conjunto de a√ß√µes poss√≠veis (sele√ß√£o de v√©rtices de patrulhamento).
- **Transi√ß√£o ($P$):** A fun√ß√£o de transi√ß√£o estoc√°stica.
- **Recompensa ($R$):** A fun√ß√£o de recompensa compartilhada.
- **Fator de Desconto ($\gamma$):** Par√¢metro que pondera a import√¢ncia de recompensas futuras.

---

# Os Agentes ($\mathcal{A}$)

<style scoped>
section {
  font-size: 25px;
}
</style>

Os **agentes** s√£o as pr√≥prias **patrulhas policiais** individuais. Suas principais caracter√≠sticas s√£o:

- **Autonomia e Descentraliza√ß√£o:** Cada patrulha decide para onde patrulhar de forma aut√¥noma, com base em sua vis√£o local do ambiente. Todos os agentes utilizam a **mesma arquitetura de rede neural** (Dueling DQN) para tomar suas decis√µes.

- **Coopera√ß√£o:** Agentes trabalham juntos, incentivados por uma **recompensa global** compartilhada, para beneficiar o sistema como um todo.

- **Pol√≠tica:** O objetivo √© encontrar uma **pol√≠tica de movimenta√ß√£o ($\pi$)** √≥tima. Essa pol√≠tica representa a estrat√©gia ideal para selecionar a sequ√™ncia de v√©rtices a serem patrulhados.

- **Vis√£o Parcial:** O agente opera com base em uma **observa√ß√£o local ($o_i$)**, um vetor de 19 informa√ß√µes, sem acesso ao estado global completo.

---

<!-- # Formula√ß√£o MARL: Estados e Observa√ß√µes -->
# Estados($\mathcal{S}$) | Observa√ß√µes($o_i$)

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40% contain](images/fig9.png)

- O **Estado ($\mathcal{S}$)** cont√©m toda a informa√ß√£o do ambiente, mas o agente decide com base apenas na **Observa√ß√£o ($o_i$)**, um subconjunto parcial de dados que entra na rede neural.

O vetor de observa√ß√£o ($o_i$) possui 19 componentes, agrupados em 4 categorias:
1.  **Informa√ß√µes Temporais:** Hora do dia e dia da semana.
2.  **M√©tricas Operacionais:** Desempenho geral do sistema (filas, ociosidade).
3.  **Dados do Pr√≥prio Agente:** Sua localiza√ß√£o, tipo e companhia.
4.  **Informa√ß√µes de Risco:** N√≠vel de risco do local atual e da sua √°rea.

---

# A√ß√µes ($\mathcal{U}$) e Transi√ß√µes ($P$)

<style scoped>
section {
  font-size: 25px;
}
/*h2 {
  padding-bottom: 15px;
}*/
</style>

### A√ß√µes
A **a√ß√£o ($u_{i,t}$)** √© a decis√£o que um agente $i$ toma no minuto $t$ quando est√° dispon√≠vel. A a√ß√£o √© a **escolha de um v√©rtice de destino** para patrulhar. As a√ß√µes de todos os agentes √© o vetor:
$\mathbf{u}_t = (u_{1,t}, u_{2,t}, \dots, u_{|\mathcal{A}|,t})$

### Transi√ß√£o
A **fun√ß√£o de transi√ß√£o** descreve como o estado evolui, gerando $s_{t+1}$ a partir de:
1. O estado atual $s_t$ (o **estado global** completo do ambiente).
2. A a√ß√£o conjunta dos agentes $\mathbf{u}_t$.
3. A **tabela de eventos ($\mathcal{E}$)**, com incidentes pr√©-gerados para um **epis√≥dio**.

$s_{t+1} \sim P(\cdot \mid s_t, \mathbf{u}_t, \mathcal{E})$

---

# Recompensa (R)

<style scoped>
section {
  font-size: 25px;
}
</style>

A recompensa √© **global e compartilhada** para incentivar a coopera√ß√£o.
$r_t = \alpha \cdot \Delta \text{atendidos}_t - \lambda_{\text{idle}} \cdot \widetilde{\Delta \text{idle}_t} - \lambda_{\text{resp}} \cdot \widetilde{\Delta \text{resp}_t} - \lambda_{\text{back}} \cdot \widetilde{\Delta \text{backlog}_t}$

- **Componentes da Recompensa:**
    - **$\Delta \text{atendidos}_t$:** Recompensa por chamados atendidos no minuto, ponderado pela prioridade. Incentiva a **efici√™ncia**.
    - **$\widetilde{\Delta \text{idle}_t}$:** Penaliza o aumento da ociosidade acumulada nos hotspots. Incentiva a **preven√ß√£o**.
    - **$\widetilde{\Delta \text{resp}_t}$:** Penaliza o aumento do tempo de resposta acumulado (ponderado por prioridade). Incentiva a **agilidade**.
    - **$\widetilde{\Delta \text{backlog}_t}$:** Penaliza o aumento de chamadas esperando na fila. Incentiva a **capacidade do sistema**.
<!-- - Os hiperpar√¢metros $\alpha$ e $\lambda$s controlam o *trade-off* entre esses objetivos. -->

\* Os termos com til ($\widetilde{\cdot}$) representam vers√µes normalizadas dos deltas.

---

# Arquitetura da Rede (Dueling DQN)

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40% contain](images/fig7.png)

A fun√ß√£o de valor $Q(o, u)$ de cada agente √© aproximada por uma rede neural (MLP) com a arquitetura **Dueling DQN**.

Esta arquitetura possui dois "fluxos" separados:
1.  **Fluxo do Valor:** Estima o qu√£o bom √© o estado atual - $V(s)$.
2.  **Fluxo da Vantagem:** Estima a vantagem de cada a√ß√£o naquele estado - $A(s, a)$.

Os dois ramos s√£o combinados para gerar os Q-values finais, o que estabiliza o aprendizado:
$Q(s, a) = V(s) + (A(s, a) - \text{mean}(A(s, a)))$


---

# Valida√ß√£o: Setup Experimental

<style scoped>
section {
  font-size: 25px;
}
</style>

## Estrat√©gias Comparadas
1.  **MARL_8:** A melhor configura√ß√£o encontrada para o modelo MARL ap√≥s v√°rios experimentos.
2.  **BAPS:** Uma heur√≠stica forte (baseada em Otimiza√ß√£o por Col√¥nia de Formigas) usada como baseline de alto desempenho.
3.  **ALEAT√ìRIO:** Uma baseline simples onde as patrulhas escolhem destinos aleatoriamente.

## Condi√ß√µes de Avalia√ß√£o
- Foram realizadas 3 execu√ß√µes de simula√ß√£o com horizonte de 7 dias.
- Para garantir uma compara√ß√£o justa, **todas as estrat√©gias foram avaliadas sob o mesmo conjunto de chamadas geradas**.

---

# Resultados: Compara√ß√£o Geral

<style scoped>
section {
  font-size: 25px;
}
.footnote {
  font-size: 14px;
  position: absolute;
  bottom: 20px;
  left: 30px;
}
</style>

<!-- ## M√©tricas de Desempenho (M√©dias de 3 Execu√ß√µes) -->
| M√©todo    | Ociosidade | Fila | Deslocamento | Tempo Resposta |
| :-------- | :---------- | :---- | :-------------- | :---------------- |
| ALEAT√ìRIO | 4843.14     | 39.52 | **7.03**        | 46.54             |
| BAPS      | **4516.59** | **27.93** | 7.10        | **35.03**         |
| MARL_8    | 4645.42     | 34.47 | 7.25            | 41.71             |

## An√°lise
<!-- - A heur√≠stica **BAPS** apresentou o melhor desempenho global. -->
- A heur√≠stica **BAPS** (guiada por risco) se confirmou como a mais eficiente no geral, alcan√ßando os menores tempos de resposta, fila e ociosidade.
- O modelo **MARL_8** superou significativamente a baseline **ALEAT√ìRIA** em todas as m√©tricas.
- O resultado do **MARL** √© promissor, pois se aproxima de uma heur√≠stica forte, validando que o agente aprendeu uma pol√≠tica coerente.

<div class="footnote">* Valores m√©dios de 3 execu√ß√µes de simula√ß√£o independentes.</div>

---

# Resultados: An√°lise por Prioridade de Fila

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ## Tempo M√©dio na Fila de Espera (em minutos) -->
| Prioridade | ALEAT√ìRIO | BAPS    | MARL_8 |
| :--- | :--- | :--- | :--- |
| **Prioridade 1 (Cr√≠tica)** | 5.04 | **0.00** | 0.06 |
| **Prioridade 2 (Interm.)** | 49.82 | 25.73 | **15.22** |
| **Prioridade 3 (Baixa)** | 48.07 | **28.82** | 40.82 |

## An√°lise
- **P1:** BAPS e MARL_8 s√£o quase √≥timos, zerando a fila para chamadas cr√≠ticas.
- **P3:** BAPS √© melhor. O MARL sacrifica o desempenho em baixa prioridade para otimizar as demais, um comportamento esperado e ajust√°vel.
- **P2:** O MARL-8 foi **superior √† heur√≠stica BAPS**, indicando que o agente aprendeu uma pol√≠tica de posicionamento mais eficaz para chamadas de prioridade intermedi√°ria.

---

# Resultados: O Achado Principal (Prioridade 2)

<style scoped>
section {
  font-size: 25px;
}
</style>

O modelo **MARL_8** apresentou um desempenho **superior** ao da heur√≠stica BAPS para chamadas de prioridade intermedi√°ria.

- **Fila M√©dia (Prioridade 2):**
    - **MARL_8:** **15.22 min**
    - **BAPS:** 25.73 min

## Hip√≥tese
A abordagem **MARL** aprendeu uma pol√≠tica de patrulhamento mais sofisticada. Ela parece ter identificado um padr√£o de posicionamento que equilibra melhor a cobertura de zonas de alto risco com a necessidade de estar pr√≥ximo a √°reas de demanda moderada, algo que a heur√≠stica, com suas regras mais r√≠gidas, n√£o captura explicitamente.

---

# Conclus√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

## Principais Conclus√µes
- A heur√≠stica **BAPS** se mostrou mais eficiente no agregado, mas o **MARL** √© altamente competitivo.
- O **MARL se destacou na prioridade 2**, indicando que aprendeu pol√≠ticas de posicionamento complexas e n√£o √≥bvias.
- √â poss√≠vel obter ganhos operacionais relevantes **ajustando apenas o padr√£o de patrulhamento** (o que o MARL faz), sem alterar as regras de despacho.

## Contribui√ß√µes
- Integra√ß√£o de patrulhamento preventivo e reativo em um √∫nico simulador.
- Formula√ß√£o do problema como MDP multiagente com recompensa multiobjetivo.
- Valida√ß√£o emp√≠rica com dados reais de uma unidade policial brasileira.

---

# Trabalhos Futuros

<style scoped>
section {
  font-size: 25px;
}
</style>

1.  **Estender o MARL para o Despacho:** Modelar o despachante tamb√©m como um agente de RL, permitindo o aprendizado de pol√≠ticas de despacho din√¢micas, em vez de usar regras fixas.

2.  **Fun√ß√µes de Recompensa Adaptativas:** Investigar recompensas que se ajustem por prioridade, para calibrar de forma mais fina o trade-off entre os diferentes n√≠veis de criticidade das chamadas.

3.  **An√°lise de Robustez e Transferibilidade:** Avaliar o desempenho do modelo em diferentes cen√°rios de demanda (e.g., eventos especiais, crises) e testar a transferibilidade das pol√≠ticas aprendidas para outras cidades ou contextos operacionais.

---

# Obrigado! üôå  
Perguntas?


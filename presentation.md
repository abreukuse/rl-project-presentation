---
marp: true
theme: gaia
paginate: true
---

# Abordagem Multiagente na combina√ß√£o de a√ß√µes de patrulhamento preventivo e de atendimento de chamadas policiais

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ![w:500 right](images/fig2.png) -->
![bg right:40%](images/fig5.png)

Moacir Almeida Sim√µes J√∫nior
Tobias de Abreu Kuse
<!-- Instituto de Inform√°tica, Universidade Federal do Rio Grande do Sul -->

---

# Resumo do Projeto

<style scoped>
section {
  font-size: 25px;
}
</style>

Este trabalho prop√µe e avalia um modelo de **Aprendizado por Refor√ßo Multiagente (MARL)** para o patrulhamento policial urbano.

- **M√©todo:** Foi integrada uma arquitetura **Dueling DQN** a um **simulador de eventos discretos** que reproduz a opera√ß√£o policial minuto a minuto.
- **Formula√ß√£o:** O problema √© um **MDP multiagente cooperativo**, onde cada patrulha (agente) aprende uma pol√≠tica de posicionamento.
- **Recompensa:** Uma fun√ß√£o **multiobjetivo** que busca conciliar metas conflitantes (tempo de resposta, cobertura de hotspots, etc.).
- **Principal Achado:** O MARL supera o patrulhamento aleat√≥rio e se aproxima de heur√≠sticas especializadas, com destaque para ocorr√™ncias de prioridade intermedi√°ria.

---

# O Problema e os Objetivos

<style scoped>
section {
  font-size: 25px;
}
</style>

## O Desafio Central
Equilibrar dois objetivos conflitantes da atividade policial:
1.  **Patrulhamento Preventivo:** Maximizar a presen√ßa policial em √°reas de alto risco (*hotspots*) para inibir crimes.
2.  **Atendimento Reativo:** Minimizar o tempo de resposta a chamadas de emerg√™ncia.

## A Solu√ß√£o Proposta
Um **sistema multiagente (MARL)** onde as patrulhas s√£o agentes aut√¥nomos que aprendem uma **pol√≠tica de patrulhamento** para otimizar ambos os objetivos simultaneamente.

---

# Metodologia: Vis√£o Geral

<style scoped>
section {
  font-size: 25px;
}
</style>

![bg right:40%](images/fig1.png)

## Ambiente de Simula√ß√£o
- Baseado em **dados reais** do 9¬∫ Batalh√£o de Pol√≠cia Militar (Porto Alegre/RS).
- Utiliza um **grid hexagonal** para representar o espa√ßo geogr√°fico.

## Identifica√ß√£o de Hotspots
- Um modelo preditivo **(XGBoost)**, treinado com dados hist√≥ricos, classifica os hex√°gonos com base no **risco de ocorr√™ncia** de eventos para as pr√≥ximas 2 horas.

---

# Metodologia: Din√¢mica da Simula√ß√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ![bg right:45%](images/fig3.png) -->

## Simula√ß√£o de Eventos Discretos
- Desenvolvida em Python (`simpy`), modela a opera√ß√£o minuto a minuto.
- **Entidade Patrulha:** Evolui por estados (Dispon√≠vel, Deslocamento, Atendimento, etc.).
- **L√≥gica do Despachante:** Regras fixas (mais pr√≥ximo para P1, da pr√≥pria √°rea para P2/P3).
- **O MARL atua** quando a patrulha est√° no estado `DISPON√çVEL`, decidindo para onde ir.
- **Eventos Aleat√≥rios:** A chegada de chamadas √© modelada por uma **distribui√ß√£o de Poisson** (para os intervalos entre chamadas). O tempo de atendimento de cada ocorr√™ncia √© sorteado de uma **distribui√ß√£o exponencial**.


<!-- ---

# Metodologia: Din√¢mica da Simula√ß√£o

<style>
/*section {
  font-size: 25px;
}
*/img[alt~="center"] {
  position: absolute;
  top: 55%;  /* Ponto central vertical (55% para dar espa√ßo ao t√≠tulo) */
  left: 50%; /* Ponto central horizontal */
  transform: translate(-50%, -50%); /* Puxa a imagem de volta pelo seu pr√≥prio centro */
  
  /* Limites para evitar corte */
  max-height: 70%; 
  max-width: 90%;
  
  object-fit: contain; /* Mant√©m a propor√ß√£o sem distorcer */
}
</style>

![center](images/fig3.png)
 -->

---

# Metodologia: Formula√ß√£o MARL

<style scoped>
section {
  font-size: 25px;
}
</style>

O problema √© formulado como um **Processo de Decis√£o de Markov (MDP) multiagente e cooperativo**.

A estrutura √© definida pela tupla:
$\mathcal{M} = \langle \mathcal{A}, \mathcal{S}, \mathcal{U}, P, R, \gamma \rangle$

Nos pr√≥ximos slides, cada um desses componentes ser√° detalhado.

---

# Formula√ß√£o MARL: Agentes (A)

<style scoped>
section {
  font-size: 25px;
}
</style>

Os **agentes** no modelo s√£o as pr√≥prias **patrulhas policiais**.

- Cada patrulha opera como um agente de decis√£o independente.
- O objetivo de cada agente √© aprender uma pol√≠tica de movimenta√ß√£o que contribua para o bem comum do sistema (maximizar a recompensa global).

---

# Formula√ß√£o MARL: Estado (S) vs. Observa√ß√£o (oi)

<style scoped>
section {
  font-size: 25px;
}
</style>

√â crucial distinguir o Estado Global da Observa√ß√£o Local de cada agente:

- **Estado Global ($\mathcal{S}$):** √â a "verdade absoluta" do simulador. Cont√©m a informa√ß√£o de **todas** as patrulhas, **todas** as chamadas na fila, o risco de todos os hotspots, etc.

- **Observa√ß√£o Local ($o_i$):** √â a vis√£o **parcial** que cada agente `i` tem do mundo. Nenhum agente v√™ tudo. Esta observa√ß√£o √© o **vetor de 19 dimens√µes** que serve de entrada para a sua rede neural.

---

# Formula√ß√£o MARL: A√ß√µes (U)

<style scoped>
section {
  font-size: 25px;
}
</style>

O **espa√ßo de a√ß√µes ($\mathcal{U}$)** define o que um agente pode fazer.

- O espa√ßo de a√ß√µes √© **discreto**.
- Uma "a√ß√£o" ($u_i$) consiste na escolha de um **v√©rtice de destino** para patrulhamento.
- Os destinos poss√≠veis s√£o todos os v√©rtices do grafo, que incluem os **pontos de refer√™ncia dos hotspots** e os **quart√©is (dep√≥sitos)**.

---

# Formula√ß√£o MARL: Transi√ß√£o (P)

<style scoped>
section {
  font-size: 25px;
}
</style>

A **fun√ß√£o de transi√ß√£o ($P$)** representa as "regras da f√≠sica" do ambiente.

- No modelo, a transi√ß√£o **√© a pr√≥pria din√¢mica do simulador**, explicada anteriormente.
- A cada minuto, o simulador:
    1. Processa as a√ß√µes escolhidas pelos agentes (inicia deslocamentos).
    2. Introduz eventos estoc√°sticos (novas chamadas que chegam da tabela de eventos $\mathcal{E}$).
    3. Atualiza o estado de todas as entidades.

---

# Formula√ß√£o MARL: Recompensa (R)

<style scoped>
section {
  font-size: 25px;
}
</style>

A recompensa √© **global e compartilhada**, refletindo o desempenho do sistema como um todo.

$r_t = \alpha \cdot \Delta \text{atendidos}_t - \lambda_{\text{idle}} \cdot \widetilde{\Delta \text{idle}_t} - \lambda_{\text{resp}} \cdot \widetilde{\Delta \text{resp}_t} - \lambda_{\text{back}} \cdot \widetilde{\Delta \text{backlog}_t}$

- **Componentes:**
    - **$\Delta \text{atendidos}$ (Positivo):** Incentiva o atendimento de chamadas.
    - **$\widetilde{\Delta \text{idle}}$ (Negativo):** Penaliza a ociosidade dos *hotspots*.
    - **$\widetilde{\Delta \text{resp}}$ (Negativo):** Penaliza o tempo de resposta √†s chamadas.
    - **$\widetilde{\Delta \text{backlog}}$ (Negativo):** Penaliza o n√∫mero de chamadas em fila.

---

# Metodologia: Treinamento da Rede Neural

<style scoped>
section {
  font-size: 25px;
}
</style>

O aprendizado ocorre atrav√©s de uma rede neural (Dueling DQN) para cada agente.

- **Entrada da Rede:** O vetor de **observa√ß√£o $o_i$** (19 valores) que descreve o que o agente `i` sabe sobre o ambiente.

- **Sa√≠da da Rede:** Um **vetor de Q-values**. Cada neur√¥nio de sa√≠da corresponde ao valor esperado de se mover para um dos v√©rtices poss√≠veis.

A a√ß√£o com o maior Q-value √© a escolhida (seguindo a pol√≠tica $\epsilon$-greedy durante o treinamento).

---

# Valida√ß√£o: Setup Experimental

<style scoped>
section {
  font-size: 25px;
}
</style>

## Estrat√©gias Comparadas
1.  **MARL_8:** A melhor configura√ß√£o encontrada para o modelo MARL ap√≥s v√°rios experimentos.
2.  **BAPS:** Uma heur√≠stica forte (baseada em Otimiza√ß√£o por Col√¥nia de Formigas) usada como baseline de alto desempenho.
3.  **ALEAT√ìRIO:** Uma baseline simples onde as patrulhas escolhem destinos aleatoriamente.

## Condi√ß√µes de Avalia√ß√£o
- Foram realizadas 3 execu√ß√µes de simula√ß√£o com horizonte de 7 dias.
- Para garantir uma compara√ß√£o justa, **todas as estrat√©gias foram avaliadas sob o mesmo conjunto de chamadas geradas**.

---

# Resultados: Compara√ß√£o Geral

<style scoped>
section {
  font-size: 25px;
}
.footnote {
  font-size: 14px;
  position: absolute;
  bottom: 20px;
  left: 30px;
}
</style>

<!-- ## M√©tricas de Desempenho (M√©dias de 3 Execu√ß√µes) -->
| M√©todo    | Ociosidade | Fila | Deslocamento | Tempo Resposta |
| :-------- | :---------- | :---- | :-------------- | :---------------- |
| ALEAT√ìRIO | 4843.14     | 39.52 | **7.03**        | 46.54             |
| BAPS      | **4516.59** | **27.93** | 7.10        | **35.03**         |
| MARL_8    | 4645.42     | 34.47 | 7.25            | 41.71             |

## An√°lise
- A heur√≠stica **BAPS** apresentou o melhor desempenho global.
- O modelo **MARL_8** superou significativamente a baseline **ALEAT√ìRIA** em todas as m√©tricas.
- O resultado do MARL √© promissor, pois se aproxima de uma heur√≠stica forte, validando que o agente aprendeu uma pol√≠tica coerente.

<div class="footnote">* Valores m√©dios de 3 execu√ß√µes de simula√ß√£o independentes.</div>

---

# Resultados: An√°lise por Prioridade de Fila

<style scoped>
section {
  font-size: 25px;
}
</style>

<!-- ## Tempo M√©dio na Fila de Espera (em minutos) -->
| Prioridade | ALEAT√ìRIO | BAPS    | MARL_8 |
| :--- | :--- | :--- | :--- |
| **Prioridade 1 (Cr√≠tica)** | 5.04 | **0.00** | 0.06 |
| **Prioridade 2 (Interm.)** | 49.82 | 25.73 | **15.22** |
| **Prioridade 3 (Baixa)** | 48.07 | **28.82** | 40.82 |

## An√°lise
- **P1:** BAPS e MARL_8 s√£o quase √≥timos, zerando a fila para chamadas cr√≠ticas.
- **P3:** BAPS √© melhor. O MARL sacrifica o desempenho em baixa prioridade para otimizar as demais, um comportamento esperado e ajust√°vel.
- **P2:** O MARL-8 foi **superior √† heur√≠stica BAPS**, indicando que o agente aprendeu uma pol√≠tica de posicionamento mais eficaz para chamadas de prioridade intermedi√°ria.

---

# Resultados: O Achado Principal (Prioridade 2)

<style scoped>
section {
  font-size: 25px;
}
</style>

O modelo **MARL_8** apresentou um desempenho **superior** ao da heur√≠stica BAPS para chamadas de prioridade intermedi√°ria.

- **Fila M√©dia (Prioridade 2):**
    - **MARL_8:** **15.22 min**
    - **BAPS:** 25.73 min

## Hip√≥tese
O agente MARL aprendeu uma pol√≠tica de patrulhamento mais sofisticada. Ele parece ter identificado um padr√£o de posicionamento que equilibra melhor a cobertura de zonas de alto risco com a necessidade de estar pr√≥ximo a √°reas de demanda moderada, algo que a heur√≠stica, com suas regras mais r√≠gidas, n√£o captura explicitamente.

---

# Conclus√£o

<style scoped>
section {
  font-size: 25px;
}
</style>

## Principais Conclus√µes
- A heur√≠stica **BAPS** se mostrou mais eficiente no agregado, mas o **MARL** √© altamente competitivo.
- O **MARL se destacou na prioridade 2**, indicando que aprendeu pol√≠ticas de posicionamento complexas e n√£o √≥bvias.
- √â poss√≠vel obter ganhos operacionais relevantes **ajustando apenas o padr√£o de patrulhamento** (o que o MARL faz), sem alterar as regras de despacho.

## Contribui√ß√µes
- Integra√ß√£o de patrulhamento preventivo e reativo em um √∫nico simulador.
- Formula√ß√£o do problema como MDP multiagente com recompensa multiobjetivo.
- Valida√ß√£o emp√≠rica com dados reais de uma unidade policial brasileira.

---

# Trabalhos Futuros

<style scoped>
section {
  font-size: 25px;
}
</style>

1.  **Estender o MARL para o Despacho:** Modelar o despachante tamb√©m como um agente de RL, permitindo o aprendizado de pol√≠ticas de despacho din√¢micas, em vez de usar regras fixas.

2.  **Fun√ß√µes de Recompensa Adaptativas:** Investigar recompensas que se ajustem por prioridade, para calibrar de forma mais fina o trade-off entre os diferentes n√≠veis de criticidade das chamadas.

3.  **An√°lise de Robustez e Transferibilidade:** Avaliar o desempenho do modelo em diferentes cen√°rios de demanda (e.g., eventos especiais, crises) e testar a transferibilidade das pol√≠ticas aprendidas para outras cidades ou contextos operacionais.

---

# Obrigado! üôå  
Perguntas?